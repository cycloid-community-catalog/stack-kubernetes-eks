---
additionalPrometheusRulesMap:
  rule-name:
    groups:
      # otheride default rules to change the for and severity
      # Add: * on (namespace) group_left() kube_namespace_annotations{annotation_prometheus_monitoring="true"}
      # So it only check pod in namespace with prometheus/monitoring=true annotation
      - name: kubernetes-apps
        rules:
          - alert: KubePodNotReady
            annotations:
              description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
                state for longer than 10 minutes.
              runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodnotready
              summary: Pod has been in a non-ready state for more than 10 minutes.
            expr: |-
              sum by (namespace, pod, cluster) (
                max by(namespace, pod, cluster) (
                  kube_pod_status_phase{job="kube-state-metrics", namespace=~".*", phase=~"Pending|Unknown|Failed"}
                ) * on(namespace, pod, cluster) group_left(owner_kind) topk by(namespace, pod, cluster) (
                  1, max by(namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!="Job"})
                ) * on (namespace) group_left() kube_namespace_annotations{annotation_prometheus_monitoring="true"}
              ) > 0
            for: 10m
            labels:
              severity: critical

          - alert: KubePodCrashLooping
            annotations:
              description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
                }}) is in waiting state (reason: "CrashLoopBackOff").'
              runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodcrashlooping
              summary: Pod is crash looping.
            expr: max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff",
              job="kube-state-metrics", namespace=~".*"}[5m]) * on (namespace) group_left() kube_namespace_annotations{annotation_prometheus_monitoring="true"} >= 1
            for: 15m
            labels:
              severity: critical

          - alert: KubeDeploymentReplicasMismatch
            annotations:
              description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has
                not matched the expected number of replicas for longer than 15 minutes.
              runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentreplicasmismatch
              summary: Deployment has not matched the expected number of replicas.
            expr: |-
              (
                kube_deployment_spec_replicas{job="kube-state-metrics", namespace=~".*"}
                  >
                kube_deployment_status_replicas_available{job="kube-state-metrics", namespace=~".*"}
              ) and (
                changes(kube_deployment_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[10m]) * on (namespace) group_left() kube_namespace_annotations{annotation_prometheus_monitoring="true"}
                  ==
                0
              )
            for: 15m
            labels:
              severity: critical

          - alert: KubeStatefulSetReplicasMismatch
            annotations:
              description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has
                not matched the expected number of replicas for longer than 15 minutes.
              runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetreplicasmismatch
              summary: Deployment has not matched the expected number of replicas.
            expr: |-
              (
                kube_statefulset_status_replicas_ready{job="kube-state-metrics", namespace=~".*"}
                  !=
                kube_statefulset_status_replicas{job="kube-state-metrics", namespace=~".*"}
              ) and (
                changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[10m]) * on (namespace) group_left() kube_namespace_annotations{annotation_prometheus_monitoring="true"}
                  ==
                0
              )
            for: 15m
            labels:
              severity: critical

          - alert: KubeContainerWaiting
            annotations:
              description: pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container
                {{ $labels.container}} has been in waiting state for longer than 1 hour.
              runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting
              summary: Pod container waiting longer than 1 hour
            expr: sum by (namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{job="kube-state-metrics",
              namespace=~".*"}) * on (namespace) group_left() kube_namespace_annotations{annotation_prometheus_monitoring="true"} > 0
            for: 1h
            labels:
              severity: critical
          - alert: KubeDaemonSetNotScheduled
            annotations:
              description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
                }} are not scheduled.'
              runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetnotscheduled
              summary: DaemonSet pods are not scheduled.
            expr: |-
              kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
                -
              kube_daemonset_status_current_number_scheduled{job="kube-state-metrics", namespace=~".*"} * on (namespace) group_left() kube_namespace_annotations{annotation_prometheus_monitoring="true"} > 0
            for: 10m
            labels:
              severity: critical
          - alert: KubeDaemonSetMisScheduled
            annotations:
              description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
                }} are running where they are not supposed to run.'
              runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetmisscheduled
              summary: DaemonSet pods are misscheduled.
            expr: kube_daemonset_status_number_misscheduled{job="kube-state-metrics", namespace=~".*"} * on (namespace) group_left() kube_namespace_annotations{annotation_prometheus_monitoring="true"}
              > 0
            for: 15m
            labels:
              severity: critical

      - name: kubernetes-ingress
        rules:
          - alert: BlackboxIngressHttpFailure
            expr: probe_http_status_code <= 199 OR probe_http_status_code >= 400
            for: 7m
            labels:
              severity: critical
            annotations:
              summary: "Ingress HTTP failure (instance {{ $labels.target }})"
              message: "HTTP status code is not 200-399\n  VALUE = {{ $value }}"



      - name: kubernetes-storage
        rules:
        - alert: KubePersistentVolumeFillingUp
          annotations:
            description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
              }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage
              }} free.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup
            summary: PersistentVolume is filling up.
          expr: |-
            (
              kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
                /
              kubelet_volume_stats_capacity_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
            ) < 0.03
            and
            kubelet_volume_stats_used_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"} * on (namespace) group_left() kube_namespace_annotations{annotation_prometheus_monitoring="true"} > 0
            unless on(namespace, persistentvolumeclaim)
            kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
            unless on(namespace, persistentvolumeclaim)
            kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
          for: 1m
          labels:
            severity: critical


        - alert: KubePersistentVolumeInodesFillingUp
          annotations:
            description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
              }} in Namespace {{ $labels.namespace }} only has {{ $value | humanizePercentage
              }} free inodes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeinodesfillingup
            summary: PersistentVolumeInodes are filling up.
          expr: |-
            (
              kubelet_volume_stats_inodes_free{job="kubelet", namespace=~".*", metrics_path="/metrics"}
                /
              kubelet_volume_stats_inodes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
            ) < 0.03
            and
            kubelet_volume_stats_inodes_used{job="kubelet", namespace=~".*", metrics_path="/metrics"} * on (namespace) group_left() kube_namespace_annotations{annotation_prometheus_monitoring="true"} > 0
            unless on(namespace, persistentvolumeclaim)
            kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
            unless on(namespace, persistentvolumeclaim)
            kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
          for: 1m
          labels:
            severity: critical

        - alert: KubePersistentVolumeErrors
          annotations:
            description: The persistent volume {{ $labels.persistentvolume }} has status
              {{ $labels.phase }}.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeerrors
            summary: PersistentVolume is having issues with provisioning.
          expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} * on (namespace) group_left() kube_namespace_annotations{annotation_prometheus_monitoring="true"}
            > 0
          for: 5m
          labels:
            severity: critical


      - name: kubernetes-system-kubelet
        rules:
        - alert: KubeNodeNotReady
          annotations:
            description: '{{ $labels.node }} has been unready for more than 15 minutes.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodenotready
            summary: Node is not ready.
          expr: kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"}
            == 0
          for: 15m
          labels:
            severity: critical

        - alert: KubeNodeUnreachable
          annotations:
            description: '{{ $labels.node }} is unreachable and some workloads may be rescheduled.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodeunreachable
            summary: Node is unreachable.
          expr: (kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"}
            unless ignoring(key,value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"})
            == 1
          for: 15m
          labels:
            severity: critical

        # - alert: KubeletTooManyPods
        #   annotations:
        #     description: Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage
        #       }} of its Pod capacity.
        #     runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubelettoomanypods
        #     summary: Kubelet is running at capacity.
        #   expr: |-
        #     count by(cluster, node) (
        #       (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job="kube-state-metrics"})
        #     )
        #     /
        #     max by(cluster, node) (
        #       kube_node_status_capacity{job="kube-state-metrics",resource="pods"} != 1
        #     ) > 0.95
        #   for: 15m
        #   labels:
        #     severity: info

      - name: general.rules
        rules:
        - alert: Watchdog
          annotations:
            description: |
              This is an alert meant to ensure that the entire alerting pipeline is functional.
              This alert is always firing, therefore it should always be firing in Alertmanager
              and always fire against a receiver. There are integrations with various notification
              mechanisms that send a notification when this alert is not firing. For example the
              "DeadMansSnitch" integration in PagerDuty.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/watchdog
            summary: An alert that should always be firing to certify that Alertmanager
              is working properly.
          expr: vector(1)
          labels:
            # customer: '{{ $labels.customer }}'
            # env: '{{ $labels.env }}'
            # project: '{{ $labels.project }}'
            receiver: opsgenie_heartbeat
            severity: critical
